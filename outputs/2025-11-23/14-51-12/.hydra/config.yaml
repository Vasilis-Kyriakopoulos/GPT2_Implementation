model:
  vocab_size: 50257
  embed_dim: 512
  num_heads: 4
  num_layers: 4
  max_len: 256
  dropout: 0.1
training:
  batch_size: 16
  epochs: 1
  lr: 0.0005
  weight_decay: 0.01
  patience: 2
data:
  dataset_name: wikitext
  dataset_config: wikitext-2-raw-v1
  context_length: 256
  num_workers: 0
