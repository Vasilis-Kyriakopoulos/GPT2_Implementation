model:
  vocab_size: 50257
  embed_dim: 256
  num_heads: 4
  num_layers: 6
  max_len: 256
  dropout: 0.1

training:
  batch_size: 16
  epochs: 10
  lr: 3e-4
  weight_decay: 0.01
  seed: 42
  device: "cuda"
  patience: 2

data:
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-raw-v1"
  context_length: 256
  num_workers: 0